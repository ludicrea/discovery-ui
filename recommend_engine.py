"""
recommend_engine.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ãã‚Œå“²ãƒ©ã‚¸ã‚ª æ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³

ã€æ©Ÿèƒ½ã€‘
1. Notionã‹ã‚‰400ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ â†’ Summary + Full Log ã‚’å–å¾—
2. æ—¥æœ¬èªå¯¾å¿œEmbeddingã§å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ SQLite ã«ä¿å­˜
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œæ°—ã«ãªã‚‹å•ã„ã€ï¼ˆ3~5å€‹ï¼‰ã‚’ Embedding â†’ ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§ä¸Šä½5ä»¶
4. ã‚¿ã‚°ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå“²å­¦è€…ãƒ»ãƒ†ãƒ¼ãƒï¼‰ã§ãƒ–ãƒ¼ã‚¹ãƒˆèª¿æ•´

ã€ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€‘
  pip install sentence-transformers requests python-dotenv numpy scikit-learn

"""

import os
import json
import sqlite3
import logging
import time
from typing import Optional, List, Dict, Tuple
from dataclasses import dataclass
from pathlib import Path

import requests
import numpy as np
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

load_dotenv()

# â”€â”€â”€ ãƒ­ã‚°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("recommend_engine.log", encoding="utf-8"),
    ],
)
log = logging.getLogger(__name__)

# â”€â”€â”€ å®šæ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NOTION_HEADERS = {
    "Authorization": f"Bearer {os.environ.get('NOTION_TOKEN', '')}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28",
}
SORETETSU_DATABASE_ID = os.getenv("SORETETSU_DATABASE_ID", "30def4a3aa6b80c0a9afd3059538c7f2")
EMBEDDING_DB_PATH = "episode_embeddings.db"
RATE_LIMIT_SLEEP = 0.4

# æ—¥æœ¬èªå¯¾å¿œã®è»½é‡Embeddingãƒ¢ãƒ‡ãƒ«
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-minilm-l12-v2"


@dataclass
class Episode:
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    notion_id: str
    title: str
    url: str
    summary: str
    full_log: str
    philosophers: List[str]
    themes: List[str]
    episode_type: str
    difficulty: str
    ludicrea_relevance: str
    embedding: Optional[np.ndarray] = None

    def to_dict(self) -> dict:
        return {
            "notion_id": self.notion_id,
            "title": self.title,
            "url": self.url,
            "summary": self.summary,
            "episode_type": self.episode_type,
            "difficulty": self.difficulty,
            "ludicrea_relevance": self.ludicrea_relevance,
            "philosophers": self.philosophers,
            "themes": self.themes,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Notion API ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_all_episodes() -> List[Episode]:
    """Notionã‹ã‚‰å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã‚€"""
    episodes = []
    cursor = None
    count = 0

    while True:
        body = {"page_size": 100}
        if cursor:
            body["start_cursor"] = cursor

        resp = requests.post(
            f"https://api.notion.com/v1/databases/{SORETETSU_DATABASE_ID}/query",
            headers=NOTION_HEADERS,
            json=body,
        )

        if resp.status_code != 200:
            log.error(f"Notion API ã‚¨ãƒ©ãƒ¼: {resp.status_code} {resp.text[:200]}")
            break

        data = resp.json()

        for page in data.get("results", []):
            ep = _parse_page(page)
            if ep:
                episodes.append(ep)
                count += 1

        log.info(f"  èª­ã¿è¾¼ã¿ä¸­... {count} ä»¶")

        if not data.get("has_more"):
            break

        cursor = data.get("next_cursor")
        time.sleep(RATE_LIMIT_SLEEP)

    log.info(f"âœ… Notionèª­ã¿è¾¼ã¿å®Œäº†: {len(episodes)} ä»¶")
    return episodes


def _parse_page(page: dict) -> Optional[Episode]:
    """Notionãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        props = page.get("properties", {})

        # ã‚¿ã‚¤ãƒˆãƒ«
        title = "".join(
            t.get("plain_text", "")
            for t in props.get("Name", {}).get("title", [])
        )

        # URL
        url = props.get("URL", {}).get("url", "")

        # Summary
        summary = "".join(
            t.get("plain_text", "")
            for t in props.get("Summary", {}).get("rich_text", [])
        )

        # Full Logï¼ˆæœ¬æ–‡ï¼‰
        full_log = _fetch_page_blocks_text(page["id"])

        # ã‚¿ã‚°æƒ…å ±
        philosophers = [
            opt["name"]
            for opt in props.get("å“²å­¦è€…", {}).get("multi_select", [])
        ]
        themes = [
            opt["name"]
            for opt in props.get("ãƒ†ãƒ¼ãƒ", {}).get("multi_select", [])
        ]
        episode_type = props.get("ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç¨®åˆ¥", {}).get("select", {}).get("name", "")
        difficulty = props.get("é›£æ˜“åº¦", {}).get("select", {}).get("name", "")
        ludicrea_relevance = props.get("ãƒ«ãƒ‡ã‚£ã‚¯ãƒ¬ã‚¢é–¢é€£åº¦", {}).get("select", {}).get("name", "")

        if not title or not url:
            return None

        return Episode(
            notion_id=page["id"],
            title=title,
            url=url,
            summary=summary,
            full_log=full_log,
            philosophers=philosophers,
            themes=themes,
            episode_type=episode_type,
            difficulty=difficulty,
            ludicrea_relevance=ludicrea_relevance,
        )
    except Exception as e:
        log.warning(f"ãƒšãƒ¼ã‚¸ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
        return None


def _fetch_page_blocks_text(page_id: str) -> str:
    """Notionãƒšãƒ¼ã‚¸ã®æœ¬æ–‡ã‚’ã™ã¹ã¦å–å¾—"""
    try:
        resp = requests.get(
            f"https://api.notion.com/v1/blocks/{page_id}/children?page_size=100",
            headers=NOTION_HEADERS,
            timeout=30,
        )
        if resp.status_code != 200:
            return ""

        texts = []
        for block in resp.json().get("results", []):
            b_type = block.get("type", "")
            for rt in block.get(b_type, {}).get("rich_text", []):
                texts.append(rt.get("plain_text", ""))

        return " ".join(texts)[:5000]  # å®¹é‡åˆ¶é™

    except Exception as e:
        log.debug(f"æœ¬æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼ [{page_id}]: {e}")
        return ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Embedding ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EmbeddingCache:
    """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®Embeddingã‚’SQLiteã§ç®¡ç†"""

    def __init__(self, db_path: str = EMBEDDING_DB_PATH):
        self.db_path = db_path
        self.model = None
        self._init_db()

    def _init_db(self):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆï¼ˆãªã‘ã‚Œã°ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS episodes (
                notion_id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                url TEXT NOT NULL,
                summary TEXT,
                full_log TEXT,
                philosophers TEXT,
                themes TEXT,
                episode_type TEXT,
                difficulty TEXT,
                ludicrea_relevance TEXT,
                embedding BLOB,
                embedding_updated_at TIMESTAMP
            )
        """)
        conn.commit()
        conn.close()

    def load_model(self):
        """Embedding ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿é…ã„ï¼‰"""
        if self.model is None:
            log.info(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {EMBEDDING_MODEL_NAME}")
            self.model = SentenceTransformer(EMBEDDING_MODEL_NAME)
            log.info("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

    def generate_and_cache_embeddings(self, episodes: List[Episode]):
        """å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®Embeddingã‚’ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        self.load_model()

        log.info(f"ğŸ“Š {len(episodes)} ä»¶ã®Embeddingç”Ÿæˆä¸­...")

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        for idx, ep in enumerate(episodes):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            cursor.execute("SELECT embedding FROM episodes WHERE notion_id = ?", (ep.notion_id,))
            cached = cursor.fetchone()

            if cached and cached[0] is not None:
                log.debug(f"   [{idx+1}/{len(episodes)}] {ep.title[:50]} (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Š)")
                continue

            # ãƒ†ã‚­ã‚¹ãƒˆçµåˆï¼šSummary + Full Logï¼ˆæœ€åˆ2000å­—ï¼‰
            text = f"{ep.summary}\n\n{ep.full_log[:2000]}"

            # Embeddingç”Ÿæˆ
            embedding = self.model.encode(text, convert_to_numpy=True)
            embedding_bytes = embedding.tobytes()

            # DBä¿å­˜
            cursor.execute("""
                INSERT OR REPLACE INTO episodes
                (notion_id, title, url, summary, full_log, philosophers, themes,
                 episode_type, difficulty, ludicrea_relevance, embedding, embedding_updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
            """, (
                ep.notion_id,
                ep.title,
                ep.url,
                ep.summary,
                ep.full_log[:2000],
                json.dumps(ep.philosophers),
                json.dumps(ep.themes),
                ep.episode_type,
                ep.difficulty,
                ep.ludicrea_relevance,
                embedding_bytes,
            ))

            if (idx + 1) % 50 == 0:
                conn.commit()
                log.info(f"   [{idx+1}/{len(episodes)}] {len(embedding)}æ¬¡å…ƒ Embedding")

        conn.commit()
        conn.close()
        log.info(f"âœ… Embeddingç”Ÿæˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº†")

    def load_all_episodes(self) -> List[Episode]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã™ã¹ã¦ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT * FROM episodes ORDER BY title")
        rows = cursor.fetchall()
        conn.close()

        episodes = []
        for row in rows:
            notion_id, title, url, summary, full_log, philosophers, themes, \
            episode_type, difficulty, ludicrea_relevance, embedding_bytes, _ = row

            embedding = None
            if embedding_bytes:
                embedding = np.frombuffer(embedding_bytes, dtype=np.float32)

            ep = Episode(
                notion_id=notion_id,
                title=title,
                url=url,
                summary=summary,
                full_log=full_log,
                philosophers=json.loads(philosophers) if philosophers else [],
                themes=json.loads(themes) if themes else [],
                episode_type=episode_type,
                difficulty=difficulty,
                ludicrea_relevance=ludicrea_relevance,
                embedding=embedding,
            )
            episodes.append(ep)

        log.info(f"ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ {len(episodes)} ä»¶èª­ã¿è¾¼ã¿")
        return episodes


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RecommendationEngine:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› â†’ æ¨è–¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰"""

    def __init__(self, episodes: List[Episode]):
        self.episodes = episodes
        self.model = SentenceTransformer(EMBEDDING_MODEL_NAME)

        # Embeddingã‚’numpyé…åˆ—ã«çµ±åˆ
        self.embedding_matrix = np.array([
            ep.embedding if ep.embedding is not None else np.zeros(384)
            for ep in episodes
        ])

    def recommend(
        self,
        questions: List[str],
        philosopher_boosts: Optional[List[str]] = None,
        theme_boosts: Optional[List[str]] = None,
        top_k: int = 5,
    ) -> List[Tuple[Episode, float]]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œæ°—ã«ãªã‚‹å•ã„ã€ã‹ã‚‰æ¨è–¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–å¾—

        Args:
            questions: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸå•ã„ï¼ˆ3~5å€‹ï¼‰
            philosopher_boosts: ãƒãƒƒãƒã™ã‚‹ã¨æ¨è–¦åº¦ã‚’ä¸Šã’ã‚‹å“²å­¦è€…ãƒªã‚¹ãƒˆ
            theme_boosts: ãƒãƒƒãƒã™ã‚‹ã¨æ¨è–¦åº¦ã‚’ä¸Šã’ã‚‹ãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆ
            top_k: æ¨è–¦ä»¶æ•°

        Returns:
            [(Episode, ã‚¹ã‚³ã‚¢), ...] ã®ãƒªã‚¹ãƒˆï¼ˆã‚¹ã‚³ã‚¢é™é †ï¼‰
        """
        if not questions:
            log.warning("è³ªå•ãŒç©ºã§ã™")
            return []

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’EmbeddingåŒ–
        user_text = " ".join(questions)
        user_embedding = self.model.encode(user_text, convert_to_numpy=True)

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
        similarities = cosine_similarity(
            user_embedding.reshape(1, -1),
            self.embedding_matrix,
        ).flatten()

        # ã‚¿ã‚°ãƒãƒƒãƒãƒ³ã‚°ã§ãƒ–ãƒ¼ã‚¹ãƒˆ
        boosts = np.ones(len(self.episodes))

        if philosopher_boosts:
            for i, ep in enumerate(self.episodes):
                if any(p in ep.philosophers for p in philosopher_boosts):
                    boosts[i] *= 1.2  # 20% ãƒ–ãƒ¼ã‚¹ãƒˆ

        if theme_boosts:
            for i, ep in enumerate(self.episodes):
                if any(t in ep.themes for t in theme_boosts):
                    boosts[i] *= 1.2

        # ã‚¹ã‚³ã‚¢ = é¡ä¼¼åº¦ Ã— ã‚¿ã‚°ãƒ–ãƒ¼ã‚¹ãƒˆ
        scores = similarities * boosts

        # ä¸Šä½kä»¶ã‚’å–å¾—
        top_indices = np.argsort(scores)[::-1][:top_k]

        results = [
            (self.episodes[idx], float(scores[idx]))
            for idx in top_indices
        ]

        return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def init_cache():
    """åˆæœŸåŒ–ï¼šNotionã‹ã‚‰èª­ã¿è¾¼ã¿ â†’ Embeddingã‚’ç”Ÿæˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    log.info("ğŸš€ æ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–é–‹å§‹")

    # Notionã‹ã‚‰å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿
    episodes = fetch_all_episodes()

    # Embeddingã‚’ç”Ÿæˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    cache = EmbeddingCache()
    cache.generate_and_cache_embeddings(episodes)

    log.info("âœ… åˆæœŸåŒ–å®Œäº†")


def get_episodes(
    philosophers: Optional[List[str]] = None,
    themes: Optional[List[str]] = None,
    search_query: str = "",
) -> Tuple[List[Episode], int]:
    """
    æœ€æ–°ä»•æ§˜ï¼šå“²å­¦è€…ãƒ»ãƒ†ãƒ¼ãƒãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
    
    Args:
        philosophers: é¸æŠã•ã‚ŒãŸå“²å­¦è€…ãƒªã‚¹ãƒˆ
        themes: é¸æŠã•ã‚ŒãŸãƒ†ãƒ¼ãƒãƒªã‚¹ãƒˆ
        search_query: ã‚µãƒ–ãƒ†ãƒ¼ãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼šã€Œç”Ÿãæ–¹ã«ã¤ã„ã¦ã€ï¼‰
    
    Returns:
        (ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ, ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¬ãƒ™ãƒ«)
    """
    cache = EmbeddingCache()
    episodes = cache.load_all_episodes()
    
    fallback_level = 0
    candidates = episodes
    
    # Level 0: ä¸¡æ–¹ãƒãƒƒãƒï¼ˆå“²å­¦è€… AND ãƒ†ãƒ¼ãƒï¼‰
    if philosophers and themes:
        candidates = [
            ep for ep in episodes
            if (any(p in ep.philosophers for p in philosophers)
                and any(t in ep.themes for t in themes))
        ]
        
        # 5å€‹æœªæº€ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if len(candidates) < 5:
            fallback_level = 1
            # Level 1: ç‰‡æ–¹ãƒãƒƒãƒï¼ˆå“²å­¦è€… OR ãƒ†ãƒ¼ãƒï¼‰
            candidates = [
                ep for ep in episodes
                if (any(p in ep.philosophers for p in philosophers)
                    or any(t in ep.themes for t in themes))
            ]
    
    # Level 1.5: å“²å­¦è€…ã¾ãŸã¯ãƒ†ãƒ¼ãƒã®ã„ãšã‚Œã‹ã®ã¿æŒ‡å®š
    elif philosophers or themes:
        candidates = [
            ep for ep in episodes
            if (any(p in ep.philosophers for p in philosophers)
                or any(t in ep.themes for t in themes))
        ]
        
        # 5å€‹æœªæº€ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if len(candidates) < 5:
            fallback_level = 2
    
    # Level 2: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆã‚µãƒ–ãƒ†ãƒ¼ãƒï¼‰
    if len(candidates) < 5 and search_query:
        fallback_level = 2
        search_lower = search_query.lower()
        candidates = [
            ep for ep in episodes
            if (search_lower in ep.title.lower()
                or search_lower in ep.summary.lower())
        ]
    
    # Level 3: ã™ã¹ã¦ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆæ–°ã—ã„é †ï¼‰
    if len(candidates) < 5:
        fallback_level = 3
        candidates = episodes
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆEmbedding ã«ã‚ˆã‚‹é¡ä¼¼åº¦ã¾ãŸã¯æ–°ã—ã„é †ï¼‰
    if search_query and candidates:
        cache.load_model()
        user_embedding = cache.model.encode(search_query, convert_to_numpy=True)
        
        scores = []
        for ep in candidates:
            if ep.embedding is not None:
                sim = cosine_similarity(
                    user_embedding.reshape(1, -1),
                    ep.embedding.reshape(1, -1)
                )[0, 0]
                scores.append(float(sim))
            else:
                scores.append(0.0)
        
        # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_indices = np.argsort(scores)[::-1]
        candidates = [candidates[i] for i in sorted_indices]
    else:
        # ã‚¿ã‚°ã®ã¿ã®å ´åˆã¯ã€Œæ–°ã—ã„é †ã€ï¼ˆå¾Œã‚ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å„ªå…ˆï¼‰
        candidates = candidates[::-1]  # é€†é †ï¼ˆæ–°ã—ã„é †ï¼‰
    
    # æœ€å¤§5å€‹ã‚’è¿”ã™
    return candidates[:5], fallback_level


def get_recommendations(
    questions: List[str],
    philosopher_boosts: Optional[List[str]] = None,
    theme_boosts: Optional[List[str]] = None,
    top_k: int = 5,
) -> List[Dict]:
    """
    ã€éæ¨å¥¨ã€‘å¤ã„æ¨è–¦ãƒ­ã‚¸ãƒƒã‚¯
    æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã¯ get_episodes() ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
    """
    cache = EmbeddingCache()
    episodes = cache.load_all_episodes()

    engine = RecommendationEngine(episodes)
    results = engine.recommend(
        questions,
        philosopher_boosts=philosopher_boosts,
        theme_boosts=theme_boosts,
        top_k=top_k,
    )

    return [
        {
            **ep.to_dict(),
            "score": float(score),
        }
        for ep, score in results
    ]


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "init":
        init_cache()
    else:
        # ãƒ†ã‚¹ãƒˆæ¨è–¦
        results = get_recommendations(
            questions=["å­˜åœ¨ã¨ã¯ä½•ã‹", "ç”Ÿæˆã¨å¤‰åŒ–ã®é–¢ä¿‚æ€§", "è¨€èªã¨æ„å‘³ã®å•é¡Œ"],
            theme_boosts=["å­˜åœ¨è«–", "è¨€èªå“²å­¦"],
        )

        log.info(f"\nğŸ“Œ æ¨è–¦çµæœï¼ˆä¸Šä½5ä»¶ï¼‰:")
        for i, res in enumerate(results, 1):
            log.info(f"  {i}. {res['title'][:60]} (ã‚¹ã‚³ã‚¢: {res['score']:.3f})")
